{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine data from chapter 4, and create a new model with the appropriate number of input parameters\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "        [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "        [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "        ...,\n",
       "        [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "        [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "        [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32),\n",
       " (4898, 12))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_path=\"./winequality-white.csv\"\n",
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=';', skiprows=1)\n",
    "wineq_numpy, wineq_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
       "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
       "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
       "         ...,\n",
       "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
       "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
       "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n",
       " tensor([6., 6., 6.,  ..., 6., 7., 6.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list = next(csv.reader(open(wine_path))) #next item from the iterator\n",
    "num_row = wineq_numpy.shape[0]\n",
    "num_col = wineq_numpy.shape[1]\n",
    "attrs = torch.tensor(np.array(wineq_numpy[:,0:11]))\n",
    "ans = torch.tensor(np.array(wineq_numpy[:,11]))\n",
    "attrs, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.array(wineq_numpy[:,11],dtype=int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_devide_set(t_u: torch, t_c:torch, ratio):\n",
    "    n_samples = t_u.shape[0]\n",
    "    n_val = int(ratio*n_samples)\n",
    "\n",
    "    shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "    train_indices = shuffled_indices[:-n_val]\n",
    "    val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "    train_indices, val_indices\n",
    "    train_t_u = t_u[train_indices]\n",
    "    train_t_c = t_c [train_indices]\n",
    "\n",
    "    val_t_u = t_u[val_indices]\n",
    "    val_t_c = t_c [val_indices]\n",
    "\n",
    "    return train_t_u, train_t_c, val_t_u, val_t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3919, 11]) torch.Size([979, 11])\n",
      "torch.Size([3919, 1]) torch.Size([979, 1])\n",
      "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
      "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])\n",
      "tensor([0.6855, 0.2782, 0.3342, 0.6391, 0.0458, 0.3531, 1.3836, 0.0994, 0.3188,\n",
      "        0.4898, 1.0514])\n",
      "tensor([[ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [-0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.3000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [-0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.3000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.2000],\n",
      "        [-0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.4000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [-0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [-0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [-0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [-0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.4000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [-0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [-0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.0000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [-0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.1000],\n",
      "        [-0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.3000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [-0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.3000],\n",
      "        [ 0.0000],\n",
      "        [ 0.3000],\n",
      "        [ 0.0000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [-0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.1000],\n",
      "        [ 0.2000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.1000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "col_norm1 = [True,False,False,True,False,False,False,True,True,False,True]\n",
    "col_norm2 = [False,False,False,False,False,True,True,False,False,False,False]\n",
    "attrs_n = attrs.clone()\n",
    "attrs_n[:,col_norm1] *= 0.1\n",
    "attrs_n[:,col_norm2] *= 0.01\n",
    "ans_n = ans*0.1-0.5\n",
    "attr_t, ans_t, attr_v, ans_v = shuffle_and_devide_set(attrs_n, ans_n, 0.2)\n",
    "ans_t = ans_t.unsqueeze(1);\n",
    "ans_v = ans_v.unsqueeze(1);\n",
    "\n",
    "print(attr_t.shape, attr_v.shape)\n",
    "print(ans_t.shape, ans_v.shape)\n",
    "print(attrs.mean(dim=0))\n",
    "print( attrs_n.mean(dim=0))\n",
    "print(ans_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_out_ans(output:torch, ans:torch) :\n",
    "    r_out = torch.round(output.clone().detach().squeeze()*10)\n",
    "    ans = torch.round(ans.clone().detach().squeeze()*10)\n",
    "    right=0\n",
    "    total=length = output.shape[0]\n",
    "    for i in range(0,length):\n",
    "        if(r_out[i] == ans[i]):\n",
    "            right+=1\n",
    "    return right, total\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs:int, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train=loss_fn(t_p_train, t_c_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch %10 or epoch % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                t_p_val = model(t_u_val)\n",
    "                loss_val = loss_fn(t_p_val, t_c_val)\n",
    "                right, total = compare_out_ans(t_p_val, t_c_val) \n",
    "                print(f\"Epoch {epoch:5d} Train Loss {loss_train.item():.4f} val loss {loss_val.item():.4f}\",\n",
    "                f\"    score: {right:3d} / {total:3d} = {right/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1 Train Loss 0.0199 val loss 0.0175     score: 217 / 979 = 22.17%\n",
      "Epoch     2 Train Loss 0.0175 val loss 0.0154     score: 248 / 979 = 25.33%\n",
      "Epoch     3 Train Loss 0.0153 val loss 0.0135     score: 284 / 979 = 29.01%\n",
      "Epoch     4 Train Loss 0.0135 val loss 0.0120     score: 306 / 979 = 31.26%\n",
      "Epoch     5 Train Loss 0.0119 val loss 0.0107     score: 339 / 979 = 34.63%\n",
      "Epoch     6 Train Loss 0.0106 val loss 0.0097     score: 373 / 979 = 38.10%\n",
      "Epoch     7 Train Loss 0.0095 val loss 0.0089     score: 403 / 979 = 41.16%\n",
      "Epoch     8 Train Loss 0.0087 val loss 0.0084     score: 403 / 979 = 41.16%\n",
      "Epoch     9 Train Loss 0.0082 val loss 0.0081     score: 418 / 979 = 42.70%\n",
      "Epoch    11 Train Loss 0.0076 val loss 0.0079     score: 430 / 979 = 43.92%\n",
      "Epoch    12 Train Loss 0.0076 val loss 0.0080     score: 437 / 979 = 44.64%\n",
      "Epoch    13 Train Loss 0.0077 val loss 0.0082     score: 432 / 979 = 44.13%\n",
      "Epoch    14 Train Loss 0.0079 val loss 0.0084     score: 432 / 979 = 44.13%\n",
      "Epoch    15 Train Loss 0.0080 val loss 0.0086     score: 426 / 979 = 43.51%\n",
      "Epoch    16 Train Loss 0.0082 val loss 0.0088     score: 418 / 979 = 42.70%\n",
      "Epoch    17 Train Loss 0.0084 val loss 0.0089     score: 413 / 979 = 42.19%\n",
      "Epoch    18 Train Loss 0.0085 val loss 0.0089     score: 410 / 979 = 41.88%\n",
      "Epoch    19 Train Loss 0.0085 val loss 0.0089     score: 410 / 979 = 41.88%\n",
      "Epoch    21 Train Loss 0.0085 val loss 0.0088     score: 415 / 979 = 42.39%\n",
      "Epoch    22 Train Loss 0.0084 val loss 0.0087     score: 426 / 979 = 43.51%\n",
      "Epoch    23 Train Loss 0.0083 val loss 0.0085     score: 428 / 979 = 43.72%\n",
      "Epoch    24 Train Loss 0.0082 val loss 0.0084     score: 434 / 979 = 44.33%\n",
      "Epoch    25 Train Loss 0.0080 val loss 0.0082     score: 435 / 979 = 44.43%\n",
      "Epoch    26 Train Loss 0.0079 val loss 0.0081     score: 433 / 979 = 44.23%\n",
      "Epoch    27 Train Loss 0.0078 val loss 0.0080     score: 445 / 979 = 45.45%\n",
      "Epoch    28 Train Loss 0.0077 val loss 0.0079     score: 434 / 979 = 44.33%\n",
      "Epoch    29 Train Loss 0.0076 val loss 0.0079     score: 436 / 979 = 44.54%\n",
      "Epoch    31 Train Loss 0.0075 val loss 0.0078     score: 432 / 979 = 44.13%\n",
      "Epoch    32 Train Loss 0.0075 val loss 0.0078     score: 431 / 979 = 44.02%\n",
      "Epoch    33 Train Loss 0.0075 val loss 0.0078     score: 435 / 979 = 44.43%\n",
      "Epoch    34 Train Loss 0.0076 val loss 0.0078     score: 424 / 979 = 43.31%\n",
      "Epoch    35 Train Loss 0.0076 val loss 0.0079     score: 424 / 979 = 43.31%\n",
      "Epoch    36 Train Loss 0.0076 val loss 0.0079     score: 418 / 979 = 42.70%\n",
      "Epoch    37 Train Loss 0.0076 val loss 0.0079     score: 418 / 979 = 42.70%\n",
      "Epoch    38 Train Loss 0.0076 val loss 0.0079     score: 420 / 979 = 42.90%\n",
      "Epoch    39 Train Loss 0.0076 val loss 0.0079     score: 418 / 979 = 42.70%\n",
      "Epoch    41 Train Loss 0.0076 val loss 0.0078     score: 420 / 979 = 42.90%\n",
      "Epoch    42 Train Loss 0.0076 val loss 0.0078     score: 426 / 979 = 43.51%\n",
      "Epoch    43 Train Loss 0.0075 val loss 0.0078     score: 429 / 979 = 43.82%\n",
      "Epoch    44 Train Loss 0.0075 val loss 0.0078     score: 433 / 979 = 44.23%\n",
      "Epoch    45 Train Loss 0.0075 val loss 0.0077     score: 434 / 979 = 44.33%\n",
      "Epoch    46 Train Loss 0.0075 val loss 0.0077     score: 432 / 979 = 44.13%\n",
      "Epoch    47 Train Loss 0.0074 val loss 0.0077     score: 435 / 979 = 44.43%\n",
      "Epoch    48 Train Loss 0.0074 val loss 0.0077     score: 432 / 979 = 44.13%\n",
      "Epoch    49 Train Loss 0.0074 val loss 0.0077     score: 438 / 979 = 44.74%\n",
      "Epoch    51 Train Loss 0.0074 val loss 0.0077     score: 439 / 979 = 44.84%\n",
      "Epoch    52 Train Loss 0.0074 val loss 0.0077     score: 439 / 979 = 44.84%\n",
      "Epoch    53 Train Loss 0.0074 val loss 0.0077     score: 440 / 979 = 44.94%\n",
      "Epoch    54 Train Loss 0.0074 val loss 0.0077     score: 443 / 979 = 45.25%\n",
      "Epoch    55 Train Loss 0.0074 val loss 0.0077     score: 443 / 979 = 45.25%\n",
      "Epoch    56 Train Loss 0.0074 val loss 0.0077     score: 443 / 979 = 45.25%\n",
      "Epoch    57 Train Loss 0.0074 val loss 0.0077     score: 444 / 979 = 45.35%\n",
      "Epoch    58 Train Loss 0.0074 val loss 0.0077     score: 443 / 979 = 45.25%\n",
      "Epoch    59 Train Loss 0.0074 val loss 0.0077     score: 440 / 979 = 44.94%\n",
      "Epoch    61 Train Loss 0.0074 val loss 0.0077     score: 438 / 979 = 44.74%\n",
      "Epoch    62 Train Loss 0.0073 val loss 0.0076     score: 436 / 979 = 44.54%\n",
      "Epoch    63 Train Loss 0.0073 val loss 0.0076     score: 436 / 979 = 44.54%\n",
      "Epoch    64 Train Loss 0.0073 val loss 0.0076     score: 437 / 979 = 44.64%\n",
      "Epoch    65 Train Loss 0.0073 val loss 0.0076     score: 436 / 979 = 44.54%\n",
      "Epoch    66 Train Loss 0.0073 val loss 0.0076     score: 433 / 979 = 44.23%\n",
      "Epoch    67 Train Loss 0.0073 val loss 0.0076     score: 433 / 979 = 44.23%\n",
      "Epoch    68 Train Loss 0.0073 val loss 0.0076     score: 434 / 979 = 44.33%\n",
      "Epoch    69 Train Loss 0.0073 val loss 0.0076     score: 434 / 979 = 44.33%\n",
      "Epoch    71 Train Loss 0.0073 val loss 0.0076     score: 432 / 979 = 44.13%\n",
      "Epoch    72 Train Loss 0.0073 val loss 0.0076     score: 432 / 979 = 44.13%\n",
      "Epoch    73 Train Loss 0.0073 val loss 0.0076     score: 433 / 979 = 44.23%\n",
      "Epoch    74 Train Loss 0.0073 val loss 0.0076     score: 433 / 979 = 44.23%\n",
      "Epoch    75 Train Loss 0.0073 val loss 0.0076     score: 433 / 979 = 44.23%\n",
      "Epoch    76 Train Loss 0.0073 val loss 0.0075     score: 433 / 979 = 44.23%\n",
      "Epoch    77 Train Loss 0.0073 val loss 0.0075     score: 434 / 979 = 44.33%\n",
      "Epoch    78 Train Loss 0.0072 val loss 0.0075     score: 432 / 979 = 44.13%\n",
      "Epoch    79 Train Loss 0.0072 val loss 0.0075     score: 432 / 979 = 44.13%\n",
      "Epoch    81 Train Loss 0.0072 val loss 0.0075     score: 435 / 979 = 44.43%\n",
      "Epoch    82 Train Loss 0.0072 val loss 0.0075     score: 436 / 979 = 44.54%\n",
      "Epoch    83 Train Loss 0.0072 val loss 0.0075     score: 436 / 979 = 44.54%\n",
      "Epoch    84 Train Loss 0.0072 val loss 0.0075     score: 437 / 979 = 44.64%\n",
      "Epoch    85 Train Loss 0.0072 val loss 0.0075     score: 438 / 979 = 44.74%\n",
      "Epoch    86 Train Loss 0.0072 val loss 0.0075     score: 439 / 979 = 44.84%\n",
      "Epoch    87 Train Loss 0.0072 val loss 0.0075     score: 440 / 979 = 44.94%\n",
      "Epoch    88 Train Loss 0.0072 val loss 0.0075     score: 440 / 979 = 44.94%\n",
      "Epoch    89 Train Loss 0.0072 val loss 0.0075     score: 438 / 979 = 44.74%\n",
      "Epoch    91 Train Loss 0.0072 val loss 0.0075     score: 438 / 979 = 44.74%\n",
      "Epoch    92 Train Loss 0.0072 val loss 0.0075     score: 438 / 979 = 44.74%\n",
      "Epoch    93 Train Loss 0.0072 val loss 0.0074     score: 438 / 979 = 44.74%\n",
      "Epoch    94 Train Loss 0.0071 val loss 0.0074     score: 436 / 979 = 44.54%\n",
      "Epoch    95 Train Loss 0.0071 val loss 0.0074     score: 436 / 979 = 44.54%\n",
      "Epoch    96 Train Loss 0.0071 val loss 0.0074     score: 438 / 979 = 44.74%\n",
      "Epoch    97 Train Loss 0.0071 val loss 0.0074     score: 439 / 979 = 44.84%\n",
      "Epoch    98 Train Loss 0.0071 val loss 0.0074     score: 438 / 979 = 44.74%\n",
      "Epoch    99 Train Loss 0.0071 val loss 0.0074     score: 435 / 979 = 44.43%\n",
      "Epoch   101 Train Loss 0.0071 val loss 0.0074     score: 435 / 979 = 44.43%\n",
      "Epoch   102 Train Loss 0.0071 val loss 0.0074     score: 435 / 979 = 44.43%\n",
      "Epoch   103 Train Loss 0.0071 val loss 0.0074     score: 435 / 979 = 44.43%\n",
      "Epoch   104 Train Loss 0.0071 val loss 0.0074     score: 436 / 979 = 44.54%\n",
      "Epoch   105 Train Loss 0.0071 val loss 0.0074     score: 437 / 979 = 44.64%\n",
      "Epoch   106 Train Loss 0.0071 val loss 0.0074     score: 438 / 979 = 44.74%\n",
      "Epoch   107 Train Loss 0.0071 val loss 0.0074     score: 441 / 979 = 45.05%\n",
      "Epoch   108 Train Loss 0.0071 val loss 0.0074     score: 442 / 979 = 45.15%\n",
      "Epoch   109 Train Loss 0.0071 val loss 0.0074     score: 439 / 979 = 44.84%\n",
      "Epoch   111 Train Loss 0.0070 val loss 0.0073     score: 440 / 979 = 44.94%\n",
      "Epoch   112 Train Loss 0.0070 val loss 0.0073     score: 440 / 979 = 44.94%\n",
      "Epoch   113 Train Loss 0.0070 val loss 0.0073     score: 440 / 979 = 44.94%\n",
      "Epoch   114 Train Loss 0.0070 val loss 0.0073     score: 440 / 979 = 44.94%\n",
      "Epoch   115 Train Loss 0.0070 val loss 0.0073     score: 440 / 979 = 44.94%\n",
      "Epoch   116 Train Loss 0.0070 val loss 0.0073     score: 441 / 979 = 45.05%\n",
      "Epoch   117 Train Loss 0.0070 val loss 0.0073     score: 441 / 979 = 45.05%\n",
      "Epoch   118 Train Loss 0.0070 val loss 0.0073     score: 440 / 979 = 44.94%\n",
      "Epoch   119 Train Loss 0.0070 val loss 0.0073     score: 440 / 979 = 44.94%\n",
      "Epoch   121 Train Loss 0.0070 val loss 0.0073     score: 437 / 979 = 44.64%\n",
      "Epoch   122 Train Loss 0.0070 val loss 0.0073     score: 437 / 979 = 44.64%\n",
      "Epoch   123 Train Loss 0.0070 val loss 0.0073     score: 437 / 979 = 44.64%\n",
      "Epoch   124 Train Loss 0.0070 val loss 0.0073     score: 437 / 979 = 44.64%\n",
      "Epoch   125 Train Loss 0.0070 val loss 0.0073     score: 437 / 979 = 44.64%\n",
      "Epoch   126 Train Loss 0.0070 val loss 0.0073     score: 438 / 979 = 44.74%\n",
      "Epoch   127 Train Loss 0.0070 val loss 0.0073     score: 438 / 979 = 44.74%\n",
      "Epoch   128 Train Loss 0.0070 val loss 0.0073     score: 438 / 979 = 44.74%\n",
      "Epoch   129 Train Loss 0.0069 val loss 0.0072     score: 439 / 979 = 44.84%\n",
      "Epoch   131 Train Loss 0.0069 val loss 0.0072     score: 440 / 979 = 44.94%\n",
      "Epoch   132 Train Loss 0.0069 val loss 0.0072     score: 440 / 979 = 44.94%\n",
      "Epoch   133 Train Loss 0.0069 val loss 0.0072     score: 440 / 979 = 44.94%\n",
      "Epoch   134 Train Loss 0.0069 val loss 0.0072     score: 440 / 979 = 44.94%\n",
      "Epoch   135 Train Loss 0.0069 val loss 0.0072     score: 442 / 979 = 45.15%\n",
      "Epoch   136 Train Loss 0.0069 val loss 0.0072     score: 443 / 979 = 45.25%\n",
      "Epoch   137 Train Loss 0.0069 val loss 0.0072     score: 443 / 979 = 45.25%\n",
      "Epoch   138 Train Loss 0.0069 val loss 0.0072     score: 443 / 979 = 45.25%\n",
      "Epoch   139 Train Loss 0.0069 val loss 0.0072     score: 444 / 979 = 45.35%\n",
      "Epoch   141 Train Loss 0.0069 val loss 0.0072     score: 445 / 979 = 45.45%\n",
      "Epoch   142 Train Loss 0.0069 val loss 0.0072     score: 446 / 979 = 45.56%\n",
      "Epoch   143 Train Loss 0.0069 val loss 0.0072     score: 447 / 979 = 45.66%\n",
      "Epoch   144 Train Loss 0.0069 val loss 0.0072     score: 447 / 979 = 45.66%\n",
      "Epoch   145 Train Loss 0.0069 val loss 0.0072     score: 447 / 979 = 45.66%\n",
      "Epoch   146 Train Loss 0.0069 val loss 0.0072     score: 447 / 979 = 45.66%\n",
      "Epoch   147 Train Loss 0.0068 val loss 0.0071     score: 447 / 979 = 45.66%\n",
      "Epoch   148 Train Loss 0.0068 val loss 0.0071     score: 446 / 979 = 45.56%\n",
      "Epoch   149 Train Loss 0.0068 val loss 0.0071     score: 446 / 979 = 45.56%\n",
      "Epoch   151 Train Loss 0.0068 val loss 0.0071     score: 446 / 979 = 45.56%\n",
      "Epoch   152 Train Loss 0.0068 val loss 0.0071     score: 446 / 979 = 45.56%\n",
      "Epoch   153 Train Loss 0.0068 val loss 0.0071     score: 446 / 979 = 45.56%\n",
      "Epoch   154 Train Loss 0.0068 val loss 0.0071     score: 447 / 979 = 45.66%\n",
      "Epoch   155 Train Loss 0.0068 val loss 0.0071     score: 448 / 979 = 45.76%\n",
      "Epoch   156 Train Loss 0.0068 val loss 0.0071     score: 447 / 979 = 45.66%\n",
      "Epoch   157 Train Loss 0.0068 val loss 0.0071     score: 448 / 979 = 45.76%\n",
      "Epoch   158 Train Loss 0.0068 val loss 0.0071     score: 448 / 979 = 45.76%\n",
      "Epoch   159 Train Loss 0.0068 val loss 0.0071     score: 447 / 979 = 45.66%\n",
      "Epoch   161 Train Loss 0.0068 val loss 0.0071     score: 448 / 979 = 45.76%\n",
      "Epoch   162 Train Loss 0.0068 val loss 0.0071     score: 448 / 979 = 45.76%\n",
      "Epoch   163 Train Loss 0.0068 val loss 0.0071     score: 448 / 979 = 45.76%\n",
      "Epoch   164 Train Loss 0.0068 val loss 0.0071     score: 448 / 979 = 45.76%\n",
      "Epoch   165 Train Loss 0.0068 val loss 0.0071     score: 448 / 979 = 45.76%\n",
      "Epoch   166 Train Loss 0.0067 val loss 0.0071     score: 449 / 979 = 45.86%\n",
      "Epoch   167 Train Loss 0.0067 val loss 0.0070     score: 448 / 979 = 45.76%\n",
      "Epoch   168 Train Loss 0.0067 val loss 0.0070     score: 450 / 979 = 45.97%\n",
      "Epoch   169 Train Loss 0.0067 val loss 0.0070     score: 450 / 979 = 45.97%\n",
      "Epoch   171 Train Loss 0.0067 val loss 0.0070     score: 450 / 979 = 45.97%\n",
      "Epoch   172 Train Loss 0.0067 val loss 0.0070     score: 450 / 979 = 45.97%\n",
      "Epoch   173 Train Loss 0.0067 val loss 0.0070     score: 450 / 979 = 45.97%\n",
      "Epoch   174 Train Loss 0.0067 val loss 0.0070     score: 450 / 979 = 45.97%\n",
      "Epoch   175 Train Loss 0.0067 val loss 0.0070     score: 450 / 979 = 45.97%\n",
      "Epoch   176 Train Loss 0.0067 val loss 0.0070     score: 451 / 979 = 46.07%\n",
      "Epoch   177 Train Loss 0.0067 val loss 0.0070     score: 453 / 979 = 46.27%\n",
      "Epoch   178 Train Loss 0.0067 val loss 0.0070     score: 453 / 979 = 46.27%\n",
      "Epoch   179 Train Loss 0.0067 val loss 0.0070     score: 453 / 979 = 46.27%\n",
      "Epoch   181 Train Loss 0.0067 val loss 0.0070     score: 452 / 979 = 46.17%\n",
      "Epoch   182 Train Loss 0.0067 val loss 0.0070     score: 452 / 979 = 46.17%\n",
      "Epoch   183 Train Loss 0.0067 val loss 0.0070     score: 452 / 979 = 46.17%\n",
      "Epoch   184 Train Loss 0.0067 val loss 0.0070     score: 451 / 979 = 46.07%\n",
      "Epoch   185 Train Loss 0.0067 val loss 0.0070     score: 451 / 979 = 46.07%\n",
      "Epoch   186 Train Loss 0.0066 val loss 0.0070     score: 451 / 979 = 46.07%\n",
      "Epoch   187 Train Loss 0.0066 val loss 0.0070     score: 452 / 979 = 46.17%\n",
      "Epoch   188 Train Loss 0.0066 val loss 0.0069     score: 453 / 979 = 46.27%\n",
      "Epoch   189 Train Loss 0.0066 val loss 0.0069     score: 453 / 979 = 46.27%\n",
      "Epoch   191 Train Loss 0.0066 val loss 0.0069     score: 452 / 979 = 46.17%\n",
      "Epoch   192 Train Loss 0.0066 val loss 0.0069     score: 452 / 979 = 46.17%\n",
      "Epoch   193 Train Loss 0.0066 val loss 0.0069     score: 453 / 979 = 46.27%\n",
      "Epoch   194 Train Loss 0.0066 val loss 0.0069     score: 454 / 979 = 46.37%\n",
      "Epoch   195 Train Loss 0.0066 val loss 0.0069     score: 453 / 979 = 46.27%\n",
      "Epoch   196 Train Loss 0.0066 val loss 0.0069     score: 453 / 979 = 46.27%\n",
      "Epoch   197 Train Loss 0.0066 val loss 0.0069     score: 455 / 979 = 46.48%\n",
      "Epoch   198 Train Loss 0.0066 val loss 0.0069     score: 456 / 979 = 46.58%\n",
      "Epoch   199 Train Loss 0.0066 val loss 0.0069     score: 456 / 979 = 46.58%\n",
      "Epoch   201 Train Loss 0.0066 val loss 0.0069     score: 457 / 979 = 46.68%\n",
      "Epoch   202 Train Loss 0.0066 val loss 0.0069     score: 458 / 979 = 46.78%\n",
      "Epoch   203 Train Loss 0.0066 val loss 0.0069     score: 458 / 979 = 46.78%\n",
      "Epoch   204 Train Loss 0.0066 val loss 0.0069     score: 457 / 979 = 46.68%\n",
      "Epoch   205 Train Loss 0.0066 val loss 0.0069     score: 457 / 979 = 46.68%\n",
      "Epoch   206 Train Loss 0.0066 val loss 0.0069     score: 457 / 979 = 46.68%\n",
      "Epoch   207 Train Loss 0.0065 val loss 0.0069     score: 457 / 979 = 46.68%\n",
      "Epoch   208 Train Loss 0.0065 val loss 0.0069     score: 457 / 979 = 46.68%\n",
      "Epoch   209 Train Loss 0.0065 val loss 0.0069     score: 458 / 979 = 46.78%\n",
      "Epoch   211 Train Loss 0.0065 val loss 0.0068     score: 458 / 979 = 46.78%\n",
      "Epoch   212 Train Loss 0.0065 val loss 0.0068     score: 459 / 979 = 46.88%\n",
      "Epoch   213 Train Loss 0.0065 val loss 0.0068     score: 460 / 979 = 46.99%\n",
      "Epoch   214 Train Loss 0.0065 val loss 0.0068     score: 460 / 979 = 46.99%\n",
      "Epoch   215 Train Loss 0.0065 val loss 0.0068     score: 460 / 979 = 46.99%\n",
      "Epoch   216 Train Loss 0.0065 val loss 0.0068     score: 460 / 979 = 46.99%\n",
      "Epoch   217 Train Loss 0.0065 val loss 0.0068     score: 460 / 979 = 46.99%\n",
      "Epoch   218 Train Loss 0.0065 val loss 0.0068     score: 460 / 979 = 46.99%\n",
      "Epoch   219 Train Loss 0.0065 val loss 0.0068     score: 461 / 979 = 47.09%\n",
      "Epoch   221 Train Loss 0.0065 val loss 0.0068     score: 462 / 979 = 47.19%\n",
      "Epoch   222 Train Loss 0.0065 val loss 0.0068     score: 462 / 979 = 47.19%\n",
      "Epoch   223 Train Loss 0.0065 val loss 0.0068     score: 464 / 979 = 47.40%\n",
      "Epoch   224 Train Loss 0.0065 val loss 0.0068     score: 465 / 979 = 47.50%\n",
      "Epoch   225 Train Loss 0.0065 val loss 0.0068     score: 465 / 979 = 47.50%\n",
      "Epoch   226 Train Loss 0.0065 val loss 0.0068     score: 467 / 979 = 47.70%\n",
      "Epoch   227 Train Loss 0.0065 val loss 0.0068     score: 465 / 979 = 47.50%\n",
      "Epoch   228 Train Loss 0.0065 val loss 0.0068     score: 464 / 979 = 47.40%\n",
      "Epoch   229 Train Loss 0.0065 val loss 0.0068     score: 464 / 979 = 47.40%\n",
      "Epoch   231 Train Loss 0.0064 val loss 0.0068     score: 464 / 979 = 47.40%\n",
      "Epoch   232 Train Loss 0.0064 val loss 0.0068     score: 463 / 979 = 47.29%\n",
      "Epoch   233 Train Loss 0.0064 val loss 0.0068     score: 464 / 979 = 47.40%\n",
      "Epoch   234 Train Loss 0.0064 val loss 0.0067     score: 464 / 979 = 47.40%\n",
      "Epoch   235 Train Loss 0.0064 val loss 0.0067     score: 467 / 979 = 47.70%\n",
      "Epoch   236 Train Loss 0.0064 val loss 0.0067     score: 467 / 979 = 47.70%\n",
      "Epoch   237 Train Loss 0.0064 val loss 0.0067     score: 468 / 979 = 47.80%\n",
      "Epoch   238 Train Loss 0.0064 val loss 0.0067     score: 468 / 979 = 47.80%\n",
      "Epoch   239 Train Loss 0.0064 val loss 0.0067     score: 469 / 979 = 47.91%\n",
      "Epoch   241 Train Loss 0.0064 val loss 0.0067     score: 468 / 979 = 47.80%\n",
      "Epoch   242 Train Loss 0.0064 val loss 0.0067     score: 467 / 979 = 47.70%\n",
      "Epoch   243 Train Loss 0.0064 val loss 0.0067     score: 467 / 979 = 47.70%\n",
      "Epoch   244 Train Loss 0.0064 val loss 0.0067     score: 467 / 979 = 47.70%\n",
      "Epoch   245 Train Loss 0.0064 val loss 0.0067     score: 467 / 979 = 47.70%\n",
      "Epoch   246 Train Loss 0.0064 val loss 0.0067     score: 468 / 979 = 47.80%\n",
      "Epoch   247 Train Loss 0.0064 val loss 0.0067     score: 469 / 979 = 47.91%\n",
      "Epoch   248 Train Loss 0.0064 val loss 0.0067     score: 469 / 979 = 47.91%\n",
      "Epoch   249 Train Loss 0.0064 val loss 0.0067     score: 469 / 979 = 47.91%\n",
      "Epoch   251 Train Loss 0.0064 val loss 0.0067     score: 470 / 979 = 48.01%\n",
      "Epoch   252 Train Loss 0.0064 val loss 0.0067     score: 470 / 979 = 48.01%\n",
      "Epoch   253 Train Loss 0.0064 val loss 0.0067     score: 470 / 979 = 48.01%\n",
      "Epoch   254 Train Loss 0.0064 val loss 0.0067     score: 470 / 979 = 48.01%\n",
      "Epoch   255 Train Loss 0.0063 val loss 0.0067     score: 471 / 979 = 48.11%\n",
      "Epoch   256 Train Loss 0.0063 val loss 0.0067     score: 471 / 979 = 48.11%\n",
      "Epoch   257 Train Loss 0.0063 val loss 0.0067     score: 469 / 979 = 47.91%\n",
      "Epoch   258 Train Loss 0.0063 val loss 0.0067     score: 468 / 979 = 47.80%\n",
      "Epoch   259 Train Loss 0.0063 val loss 0.0067     score: 468 / 979 = 47.80%\n",
      "Epoch   261 Train Loss 0.0063 val loss 0.0066     score: 467 / 979 = 47.70%\n",
      "Epoch   262 Train Loss 0.0063 val loss 0.0066     score: 467 / 979 = 47.70%\n",
      "Epoch   263 Train Loss 0.0063 val loss 0.0066     score: 467 / 979 = 47.70%\n",
      "Epoch   264 Train Loss 0.0063 val loss 0.0066     score: 467 / 979 = 47.70%\n",
      "Epoch   265 Train Loss 0.0063 val loss 0.0066     score: 467 / 979 = 47.70%\n",
      "Epoch   266 Train Loss 0.0063 val loss 0.0066     score: 467 / 979 = 47.70%\n",
      "Epoch   267 Train Loss 0.0063 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   268 Train Loss 0.0063 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   269 Train Loss 0.0063 val loss 0.0066     score: 470 / 979 = 48.01%\n",
      "Epoch   271 Train Loss 0.0063 val loss 0.0066     score: 471 / 979 = 48.11%\n",
      "Epoch   272 Train Loss 0.0063 val loss 0.0066     score: 471 / 979 = 48.11%\n",
      "Epoch   273 Train Loss 0.0063 val loss 0.0066     score: 470 / 979 = 48.01%\n",
      "Epoch   274 Train Loss 0.0063 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   275 Train Loss 0.0063 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   276 Train Loss 0.0063 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   277 Train Loss 0.0063 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   278 Train Loss 0.0063 val loss 0.0066     score: 468 / 979 = 47.80%\n",
      "Epoch   279 Train Loss 0.0063 val loss 0.0066     score: 467 / 979 = 47.70%\n",
      "Epoch   281 Train Loss 0.0063 val loss 0.0066     score: 468 / 979 = 47.80%\n",
      "Epoch   282 Train Loss 0.0062 val loss 0.0066     score: 468 / 979 = 47.80%\n",
      "Epoch   283 Train Loss 0.0062 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   284 Train Loss 0.0062 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   285 Train Loss 0.0062 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   286 Train Loss 0.0062 val loss 0.0066     score: 469 / 979 = 47.91%\n",
      "Epoch   287 Train Loss 0.0062 val loss 0.0066     score: 470 / 979 = 48.01%\n",
      "Epoch   288 Train Loss 0.0062 val loss 0.0066     score: 471 / 979 = 48.11%\n",
      "Epoch   289 Train Loss 0.0062 val loss 0.0065     score: 472 / 979 = 48.21%\n",
      "Epoch   291 Train Loss 0.0062 val loss 0.0065     score: 474 / 979 = 48.42%\n",
      "Epoch   292 Train Loss 0.0062 val loss 0.0065     score: 474 / 979 = 48.42%\n",
      "Epoch   293 Train Loss 0.0062 val loss 0.0065     score: 474 / 979 = 48.42%\n",
      "Epoch   294 Train Loss 0.0062 val loss 0.0065     score: 474 / 979 = 48.42%\n",
      "Epoch   295 Train Loss 0.0062 val loss 0.0065     score: 474 / 979 = 48.42%\n",
      "Epoch   296 Train Loss 0.0062 val loss 0.0065     score: 476 / 979 = 48.62%\n",
      "Epoch   297 Train Loss 0.0062 val loss 0.0065     score: 477 / 979 = 48.72%\n",
      "Epoch   298 Train Loss 0.0062 val loss 0.0065     score: 478 / 979 = 48.83%\n",
      "Epoch   299 Train Loss 0.0062 val loss 0.0065     score: 479 / 979 = 48.93%\n",
      "Epoch   301 Train Loss 0.0062 val loss 0.0065     score: 481 / 979 = 49.13%\n",
      "Epoch   302 Train Loss 0.0062 val loss 0.0065     score: 481 / 979 = 49.13%\n",
      "Epoch   303 Train Loss 0.0062 val loss 0.0065     score: 481 / 979 = 49.13%\n",
      "Epoch   304 Train Loss 0.0062 val loss 0.0065     score: 480 / 979 = 49.03%\n",
      "Epoch   305 Train Loss 0.0062 val loss 0.0065     score: 480 / 979 = 49.03%\n",
      "Epoch   306 Train Loss 0.0062 val loss 0.0065     score: 480 / 979 = 49.03%\n",
      "Epoch   307 Train Loss 0.0062 val loss 0.0065     score: 480 / 979 = 49.03%\n",
      "Epoch   308 Train Loss 0.0062 val loss 0.0065     score: 480 / 979 = 49.03%\n",
      "Epoch   309 Train Loss 0.0062 val loss 0.0065     score: 481 / 979 = 49.13%\n",
      "Epoch   311 Train Loss 0.0062 val loss 0.0065     score: 483 / 979 = 49.34%\n",
      "Epoch   312 Train Loss 0.0062 val loss 0.0065     score: 483 / 979 = 49.34%\n",
      "Epoch   313 Train Loss 0.0062 val loss 0.0065     score: 483 / 979 = 49.34%\n",
      "Epoch   314 Train Loss 0.0061 val loss 0.0065     score: 482 / 979 = 49.23%\n",
      "Epoch   315 Train Loss 0.0061 val loss 0.0065     score: 482 / 979 = 49.23%\n",
      "Epoch   316 Train Loss 0.0061 val loss 0.0065     score: 483 / 979 = 49.34%\n",
      "Epoch   317 Train Loss 0.0061 val loss 0.0065     score: 483 / 979 = 49.34%\n",
      "Epoch   318 Train Loss 0.0061 val loss 0.0065     score: 484 / 979 = 49.44%\n",
      "Epoch   319 Train Loss 0.0061 val loss 0.0065     score: 484 / 979 = 49.44%\n",
      "Epoch   321 Train Loss 0.0061 val loss 0.0065     score: 484 / 979 = 49.44%\n",
      "Epoch   322 Train Loss 0.0061 val loss 0.0065     score: 484 / 979 = 49.44%\n",
      "Epoch   323 Train Loss 0.0061 val loss 0.0064     score: 485 / 979 = 49.54%\n",
      "Epoch   324 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   325 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   326 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   327 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   328 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   329 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   331 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   332 Train Loss 0.0061 val loss 0.0064     score: 485 / 979 = 49.54%\n",
      "Epoch   333 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   334 Train Loss 0.0061 val loss 0.0064     score: 485 / 979 = 49.54%\n",
      "Epoch   335 Train Loss 0.0061 val loss 0.0064     score: 485 / 979 = 49.54%\n",
      "Epoch   336 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   337 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   338 Train Loss 0.0061 val loss 0.0064     score: 485 / 979 = 49.54%\n",
      "Epoch   339 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   341 Train Loss 0.0061 val loss 0.0064     score: 485 / 979 = 49.54%\n",
      "Epoch   342 Train Loss 0.0061 val loss 0.0064     score: 485 / 979 = 49.54%\n",
      "Epoch   343 Train Loss 0.0061 val loss 0.0064     score: 485 / 979 = 49.54%\n",
      "Epoch   344 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   345 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   346 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   347 Train Loss 0.0061 val loss 0.0064     score: 484 / 979 = 49.44%\n",
      "Epoch   348 Train Loss 0.0061 val loss 0.0064     score: 482 / 979 = 49.23%\n",
      "Epoch   349 Train Loss 0.0061 val loss 0.0064     score: 481 / 979 = 49.13%\n",
      "Epoch   351 Train Loss 0.0061 val loss 0.0064     score: 481 / 979 = 49.13%\n",
      "Epoch   352 Train Loss 0.0061 val loss 0.0064     score: 481 / 979 = 49.13%\n",
      "Epoch   353 Train Loss 0.0060 val loss 0.0064     score: 481 / 979 = 49.13%\n",
      "Epoch   354 Train Loss 0.0060 val loss 0.0064     score: 481 / 979 = 49.13%\n",
      "Epoch   355 Train Loss 0.0060 val loss 0.0064     score: 481 / 979 = 49.13%\n",
      "Epoch   356 Train Loss 0.0060 val loss 0.0064     score: 481 / 979 = 49.13%\n",
      "Epoch   357 Train Loss 0.0060 val loss 0.0064     score: 482 / 979 = 49.23%\n",
      "Epoch   358 Train Loss 0.0060 val loss 0.0064     score: 482 / 979 = 49.23%\n",
      "Epoch   359 Train Loss 0.0060 val loss 0.0064     score: 482 / 979 = 49.23%\n",
      "Epoch   361 Train Loss 0.0060 val loss 0.0064     score: 481 / 979 = 49.13%\n",
      "Epoch   362 Train Loss 0.0060 val loss 0.0064     score: 482 / 979 = 49.23%\n",
      "Epoch   363 Train Loss 0.0060 val loss 0.0064     score: 482 / 979 = 49.23%\n",
      "Epoch   364 Train Loss 0.0060 val loss 0.0064     score: 482 / 979 = 49.23%\n",
      "Epoch   365 Train Loss 0.0060 val loss 0.0063     score: 482 / 979 = 49.23%\n",
      "Epoch   366 Train Loss 0.0060 val loss 0.0063     score: 481 / 979 = 49.13%\n",
      "Epoch   367 Train Loss 0.0060 val loss 0.0063     score: 481 / 979 = 49.13%\n",
      "Epoch   368 Train Loss 0.0060 val loss 0.0063     score: 481 / 979 = 49.13%\n",
      "Epoch   369 Train Loss 0.0060 val loss 0.0063     score: 481 / 979 = 49.13%\n",
      "Epoch   371 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   372 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   373 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   374 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   375 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   376 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   377 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   378 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   379 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   381 Train Loss 0.0060 val loss 0.0063     score: 479 / 979 = 48.93%\n",
      "Epoch   382 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   383 Train Loss 0.0060 val loss 0.0063     score: 479 / 979 = 48.93%\n",
      "Epoch   384 Train Loss 0.0060 val loss 0.0063     score: 479 / 979 = 48.93%\n",
      "Epoch   385 Train Loss 0.0060 val loss 0.0063     score: 478 / 979 = 48.83%\n",
      "Epoch   386 Train Loss 0.0060 val loss 0.0063     score: 478 / 979 = 48.83%\n",
      "Epoch   387 Train Loss 0.0060 val loss 0.0063     score: 478 / 979 = 48.83%\n",
      "Epoch   388 Train Loss 0.0060 val loss 0.0063     score: 478 / 979 = 48.83%\n",
      "Epoch   389 Train Loss 0.0060 val loss 0.0063     score: 479 / 979 = 48.93%\n",
      "Epoch   391 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   392 Train Loss 0.0060 val loss 0.0063     score: 480 / 979 = 49.03%\n",
      "Epoch   393 Train Loss 0.0060 val loss 0.0063     score: 481 / 979 = 49.13%\n",
      "Epoch   394 Train Loss 0.0060 val loss 0.0063     score: 481 / 979 = 49.13%\n",
      "Epoch   395 Train Loss 0.0060 val loss 0.0063     score: 482 / 979 = 49.23%\n",
      "Epoch   396 Train Loss 0.0060 val loss 0.0063     score: 482 / 979 = 49.23%\n",
      "Epoch   397 Train Loss 0.0060 val loss 0.0063     score: 483 / 979 = 49.34%\n",
      "Epoch   398 Train Loss 0.0060 val loss 0.0063     score: 483 / 979 = 49.34%\n",
      "Epoch   399 Train Loss 0.0060 val loss 0.0063     score: 482 / 979 = 49.23%\n",
      "Epoch   401 Train Loss 0.0060 val loss 0.0063     score: 483 / 979 = 49.34%\n",
      "Epoch   402 Train Loss 0.0060 val loss 0.0063     score: 484 / 979 = 49.44%\n",
      "Epoch   403 Train Loss 0.0060 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   404 Train Loss 0.0060 val loss 0.0063     score: 486 / 979 = 49.64%\n",
      "Epoch   405 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   406 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   407 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   408 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   409 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   411 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   412 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   413 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   414 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   415 Train Loss 0.0059 val loss 0.0063     score: 486 / 979 = 49.64%\n",
      "Epoch   416 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   417 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   418 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   419 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   421 Train Loss 0.0059 val loss 0.0063     score: 485 / 979 = 49.54%\n",
      "Epoch   422 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   423 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   424 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   425 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   426 Train Loss 0.0059 val loss 0.0062     score: 487 / 979 = 49.74%\n",
      "Epoch   427 Train Loss 0.0059 val loss 0.0062     score: 488 / 979 = 49.85%\n",
      "Epoch   428 Train Loss 0.0059 val loss 0.0062     score: 487 / 979 = 49.74%\n",
      "Epoch   429 Train Loss 0.0059 val loss 0.0062     score: 487 / 979 = 49.74%\n",
      "Epoch   431 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   432 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   433 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   434 Train Loss 0.0059 val loss 0.0062     score: 485 / 979 = 49.54%\n",
      "Epoch   435 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   436 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   437 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   438 Train Loss 0.0059 val loss 0.0062     score: 485 / 979 = 49.54%\n",
      "Epoch   439 Train Loss 0.0059 val loss 0.0062     score: 485 / 979 = 49.54%\n",
      "Epoch   441 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   442 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   443 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   444 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   445 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   446 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   447 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   448 Train Loss 0.0059 val loss 0.0062     score: 485 / 979 = 49.54%\n",
      "Epoch   449 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   451 Train Loss 0.0059 val loss 0.0062     score: 487 / 979 = 49.74%\n",
      "Epoch   452 Train Loss 0.0059 val loss 0.0062     score: 487 / 979 = 49.74%\n",
      "Epoch   453 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   454 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   455 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   456 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   457 Train Loss 0.0059 val loss 0.0062     score: 487 / 979 = 49.74%\n",
      "Epoch   458 Train Loss 0.0059 val loss 0.0062     score: 485 / 979 = 49.54%\n",
      "Epoch   459 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   461 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   462 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   463 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   464 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   465 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   466 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   467 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   468 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   469 Train Loss 0.0059 val loss 0.0062     score: 485 / 979 = 49.54%\n",
      "Epoch   471 Train Loss 0.0059 val loss 0.0062     score: 485 / 979 = 49.54%\n",
      "Epoch   472 Train Loss 0.0059 val loss 0.0062     score: 485 / 979 = 49.54%\n",
      "Epoch   473 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   474 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   475 Train Loss 0.0059 val loss 0.0062     score: 486 / 979 = 49.64%\n",
      "Epoch   476 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   477 Train Loss 0.0059 val loss 0.0062     score: 484 / 979 = 49.44%\n",
      "Epoch   478 Train Loss 0.0059 val loss 0.0062     score: 483 / 979 = 49.34%\n",
      "Epoch   479 Train Loss 0.0059 val loss 0.0062     score: 483 / 979 = 49.34%\n",
      "Epoch   481 Train Loss 0.0059 val loss 0.0062     score: 483 / 979 = 49.34%\n",
      "Epoch   482 Train Loss 0.0059 val loss 0.0062     score: 483 / 979 = 49.34%\n",
      "Epoch   483 Train Loss 0.0059 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   484 Train Loss 0.0059 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   485 Train Loss 0.0059 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   486 Train Loss 0.0059 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   487 Train Loss 0.0059 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   488 Train Loss 0.0059 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   489 Train Loss 0.0059 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   491 Train Loss 0.0058 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   492 Train Loss 0.0058 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   493 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   494 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   495 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   496 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   497 Train Loss 0.0058 val loss 0.0062     score: 483 / 979 = 49.34%\n",
      "Epoch   498 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   499 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   500 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   501 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   502 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   503 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   504 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   505 Train Loss 0.0058 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   506 Train Loss 0.0058 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   507 Train Loss 0.0058 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   508 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   509 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   511 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   512 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   513 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   514 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   515 Train Loss 0.0058 val loss 0.0062     score: 482 / 979 = 49.23%\n",
      "Epoch   516 Train Loss 0.0058 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   517 Train Loss 0.0058 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   518 Train Loss 0.0058 val loss 0.0062     score: 481 / 979 = 49.13%\n",
      "Epoch   519 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   521 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   522 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   523 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   524 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   525 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   526 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   527 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   528 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   529 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   531 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   532 Train Loss 0.0058 val loss 0.0061     score: 482 / 979 = 49.23%\n",
      "Epoch   533 Train Loss 0.0058 val loss 0.0061     score: 482 / 979 = 49.23%\n",
      "Epoch   534 Train Loss 0.0058 val loss 0.0061     score: 482 / 979 = 49.23%\n",
      "Epoch   535 Train Loss 0.0058 val loss 0.0061     score: 482 / 979 = 49.23%\n",
      "Epoch   536 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   537 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   538 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   539 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   541 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   542 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   543 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   544 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   545 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   546 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   547 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   548 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   549 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   551 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   552 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   553 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   554 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   555 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   556 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   557 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   558 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   559 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   561 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   562 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   563 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   564 Train Loss 0.0058 val loss 0.0061     score: 482 / 979 = 49.23%\n",
      "Epoch   565 Train Loss 0.0058 val loss 0.0061     score: 482 / 979 = 49.23%\n",
      "Epoch   566 Train Loss 0.0058 val loss 0.0061     score: 482 / 979 = 49.23%\n",
      "Epoch   567 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   568 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   569 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   571 Train Loss 0.0058 val loss 0.0061     score: 482 / 979 = 49.23%\n",
      "Epoch   572 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   573 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   574 Train Loss 0.0058 val loss 0.0061     score: 481 / 979 = 49.13%\n",
      "Epoch   575 Train Loss 0.0058 val loss 0.0061     score: 482 / 979 = 49.23%\n",
      "Epoch   576 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   577 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   578 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   579 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   581 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   582 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   583 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   584 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   585 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   586 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   587 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   588 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   589 Train Loss 0.0058 val loss 0.0061     score: 483 / 979 = 49.34%\n",
      "Epoch   591 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   592 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   593 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   594 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   595 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   596 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   597 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   598 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   599 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   601 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   602 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   603 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   604 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   605 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   606 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   607 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   608 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   609 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   611 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   612 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   613 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   614 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   615 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   616 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   617 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   618 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   619 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   621 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   622 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   623 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   624 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   625 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   626 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   627 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   628 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   629 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   631 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   632 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   633 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   634 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   635 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   636 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   637 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   638 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   639 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   641 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   642 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   643 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   644 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   645 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   646 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   647 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   648 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   649 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   651 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   652 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   653 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   654 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   655 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   656 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   657 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   658 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   659 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   661 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   662 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   663 Train Loss 0.0058 val loss 0.0061     score: 484 / 979 = 49.44%\n",
      "Epoch   664 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   665 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   666 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   667 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   668 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   669 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   671 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   672 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   673 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   674 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   675 Train Loss 0.0058 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   676 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   677 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   678 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   679 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   681 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   682 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   683 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   684 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   685 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   686 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   687 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   688 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   689 Train Loss 0.0058 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   691 Train Loss 0.0058 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   692 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   693 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   694 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   695 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   696 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   697 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   698 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   699 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   701 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   702 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   703 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   704 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   705 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   706 Train Loss 0.0057 val loss 0.0061     score: 485 / 979 = 49.54%\n",
      "Epoch   707 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   708 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   709 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   711 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   712 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   713 Train Loss 0.0057 val loss 0.0061     score: 486 / 979 = 49.64%\n",
      "Epoch   714 Train Loss 0.0057 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   715 Train Loss 0.0057 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   716 Train Loss 0.0057 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   717 Train Loss 0.0057 val loss 0.0061     score: 487 / 979 = 49.74%\n",
      "Epoch   718 Train Loss 0.0057 val loss 0.0061     score: 488 / 979 = 49.85%\n",
      "Epoch   719 Train Loss 0.0057 val loss 0.0061     score: 488 / 979 = 49.85%\n",
      "Epoch   721 Train Loss 0.0057 val loss 0.0061     score: 488 / 979 = 49.85%\n",
      "Epoch   722 Train Loss 0.0057 val loss 0.0061     score: 488 / 979 = 49.85%\n",
      "Epoch   723 Train Loss 0.0057 val loss 0.0061     score: 488 / 979 = 49.85%\n",
      "Epoch   724 Train Loss 0.0057 val loss 0.0061     score: 488 / 979 = 49.85%\n",
      "Epoch   725 Train Loss 0.0057 val loss 0.0061     score: 488 / 979 = 49.85%\n",
      "Epoch   726 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   727 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   728 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   729 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   731 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   732 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   733 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   734 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   735 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   736 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   737 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   738 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   739 Train Loss 0.0057 val loss 0.0061     score: 489 / 979 = 49.95%\n",
      "Epoch   741 Train Loss 0.0057 val loss 0.0061     score: 488 / 979 = 49.85%\n",
      "Epoch   742 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   743 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   744 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   745 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   746 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   747 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   748 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   749 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   751 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   752 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   753 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   754 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   755 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   756 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   757 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   758 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   759 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   761 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   762 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   763 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   764 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   765 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   766 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   767 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   768 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   769 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   771 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   772 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   773 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   774 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   775 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   776 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   777 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   778 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   779 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   781 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   782 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   783 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   784 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   785 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   786 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   787 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   788 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   789 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   791 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   792 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   793 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   794 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   795 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   796 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   797 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   798 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   799 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   801 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   802 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   803 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   804 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   805 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   806 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   807 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   808 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   809 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   811 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   812 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   813 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   814 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   815 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   816 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   817 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   818 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   819 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   821 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   822 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   823 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   824 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   825 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   826 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   827 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   828 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   829 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   831 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   832 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   833 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   834 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   835 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   836 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   837 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   838 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   839 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   841 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   842 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   843 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   844 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   845 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   846 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   847 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   848 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   849 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   851 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   852 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   853 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   854 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   855 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   856 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   857 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   858 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   859 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   861 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   862 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   863 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   864 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   865 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   866 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   867 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   868 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   869 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   871 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   872 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   873 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   874 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   875 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   876 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   877 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   878 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   879 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   881 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   882 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   883 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   884 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   885 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   886 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   887 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   888 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   889 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   891 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   892 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   893 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   894 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   895 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   896 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   897 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   898 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   899 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   901 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   902 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   903 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   904 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   905 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   906 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   907 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   908 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   909 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   911 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   912 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   913 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   914 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   915 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   916 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   917 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   918 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   919 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   921 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   922 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   923 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   924 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   925 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   926 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   927 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   928 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   929 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   931 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   932 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   933 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   934 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   935 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   936 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   937 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   938 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   939 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   941 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   942 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   943 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   944 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   945 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   946 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   947 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   948 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   949 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   951 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   952 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   953 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   954 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   955 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   956 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   957 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   958 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   959 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   961 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   962 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   963 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   964 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   965 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   966 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   967 Train Loss 0.0057 val loss 0.0060     score: 486 / 979 = 49.64%\n",
      "Epoch   968 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   969 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   971 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   972 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   973 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   974 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   975 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   976 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   977 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   978 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   979 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   981 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   982 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   983 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   984 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   985 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   986 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   987 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   988 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   989 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   991 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   992 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   993 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   994 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   995 Train Loss 0.0057 val loss 0.0060     score: 487 / 979 = 49.74%\n",
      "Epoch   996 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   997 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   998 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch   999 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n",
      "Epoch  1000 Train Loss 0.0057 val loss 0.0060     score: 488 / 979 = 49.85%\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "    ('hidden_linear', nn.Linear(11,11)),\n",
    "    ('hidden_activation', nn.Tanh()),\n",
    "    ('hidden_linear', nn.Linear(11,6)),\n",
    "    ('hidden_activation', nn.Tanh()),\n",
    "    ('output_linear', nn.Linear(6,1))\n",
    "       \n",
    "]))\n",
    "attr_t = attr_t.clone().detach().requires_grad_(True)\n",
    "attr_v = attr_v.clone().detach().requires_grad_(True)\n",
    "ans_t =  ans_t.clone().detach().requires_grad_(True)\n",
    "ans_v =  ans_v.clone().detach().requires_grad_(True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr =1e-3)\n",
    "loss_fn = nn.MSELoss() \n",
    "training_loop(\n",
    "    1000, optimizer, \n",
    "    model, loss_fn, \n",
    "    attr_t, attr_v,\n",
    "    ans_t, ans_v\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,  30.,   0., 209.,   0., 496., 223.,   0.,  15.,   3.]),\n",
       " array([-3. , -2.4, -1.8, -1.2, -0.6,  0. ,  0.6,  1.2,  1.8,  2.4,  3. ],\n",
       "       dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOQUlEQVR4nO3df4xlZX3H8fdHlh/GX6sw3Wx3Nx0TN21JU9FMKI2msVAtP4xLEyAQoyvdZmOCKcY2utWkxLYmkCYiNg3ppku6NFQhImEjtIUCxvoH6ICIyGKdEsjuBtgRASXENsi3f9xn7bDO7vy6s3fm4f1KJnPOc87MeU52eXM499xLqgpJUl9eM+oJSJKGz7hLUoeMuyR1yLhLUoeMuyR1aM2oJwBwyimn1Pj4+KinIUmryv333/+jqhqbbduKiPv4+DiTk5OjnoYkrSpJnjjSNm/LSFKHjLskdci4S1KHjLskdWhecU/yeJLvJXkwyWQbe0uSO5P8sH1/cxtPki8mmUryUJJ3LucJSJJ+2UKu3H+/qk6rqom2vgO4q6o2A3e1dYBzgM3taztw7bAmK0man6XcltkC7G7Lu4HzZ4xfXwP3AmuTrF/CcSRJCzTfuBdwR5L7k2xvY+uq6sm2/BSwri1vAPbN+Nn9bewVkmxPMplkcnp6ehFTlyQdyXzfxPTuqjqQ5FeAO5M8OnNjVVWSBX0wfFXtBHYCTExM+KHykjRE84p7VR1o3w8muQU4HXg6yfqqerLddjnYdj8AbJrx4xvbmLTqjO+4bWTHfvzK80Z2bK1+c96WSfK6JG84tAy8D3gY2ANsbbttBW5ty3uAD7enZs4Anp9x+0aSdAzM58p9HXBLkkP7/0tV/VuSbwM3JdkGPAFc1Pa/HTgXmAJeBC4d+qwlSUc1Z9yr6jHg7bOMPwOcNct4AZcNZXaSpEXxHaqS1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1KF5xz3JcUm+k+Rrbf2tSe5LMpXkxiQntPET2/pU2z6+THOXJB3BQq7cLwf2zli/Cri6qt4GPAtsa+PbgGfb+NVtP0nSMTSvuCfZCJwH/GNbD3Am8JW2y27g/La8pa3Ttp/V9pckHSPzvXL/AvBJ4OW2fjLwXFW91Nb3Axva8gZgH0Db/nzb/xWSbE8ymWRyenp6cbOXJM1qzrgneT9wsKruH+aBq2pnVU1U1cTY2Ngwf7Ukveqtmcc+7wI+kORc4CTgjcA1wNoka9rV+UbgQNv/ALAJ2J9kDfAm4Jmhz1ySdERzXrlX1V9U1caqGgcuBu6uqg8C9wAXtN22Are25T1tnbb97qqqoc5aknRUS3nO/VPAJ5JMMbinvquN7wJObuOfAHYsbYqSpIWaz22ZX6iqrwNfb8uPAafPss/PgAuHMDdJ0iL5DlVJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QOzRn3JCcl+VaS7yb5fpLPtvG3JrkvyVSSG5Oc0MZPbOtTbfv4Mp+DJOkw87ly/x/gzKp6O3AacHaSM4CrgKur6m3As8C2tv824Nk2fnXbT5J0DM0Z9xp4oa0e374KOBP4ShvfDZzflre0ddr2s5JkWBOWJM1tXvfckxyX5EHgIHAn8N/Ac1X1UttlP7ChLW8A9gG07c8DJw9xzpKkOcwr7lX186o6DdgInA78xlIPnGR7kskkk9PT00v9dZKkGRb0tExVPQfcA/wusDbJmrZpI3CgLR8ANgG07W8Cnpnld+2sqomqmhgbG1vc7CVJs5rP0zJjSda25dcC7wX2Moj8BW23rcCtbXlPW6dtv7uqaohzliTNYc3cu7Ae2J3kOAb/Mripqr6W5BHgy0n+BvgOsKvtvwv45yRTwI+Bi5dh3pKko5gz7lX1EPCOWcYfY3D//fDxnwEXDmV2kqRF8R2qktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktShNaOegKTZje+4bSTHffzK80ZyXA2XV+6S1CHjLkkdMu6S1CHjLkkdMu6S1KE5n5ZJsgm4HlgHFLCzqq5J8hbgRmAceBy4qKqeTRLgGuBc4EXgI1X1wPJMX8eaT3BIq8N8rtxfAv6sqk4FzgAuS3IqsAO4q6o2A3e1dYBzgM3taztw7dBnLUk6qjnjXlVPHrryrqqfAnuBDcAWYHfbbTdwflveAlxfA/cCa5OsH/bEJUlHtqB77knGgXcA9wHrqurJtukpBrdtYBD+fTN+bH8bO/x3bU8ymWRyenp6ofOWJB3FvOOe5PXAzcDHq+onM7dVVTG4Hz9vVbWzqiaqamJsbGwhPypJmsO84p7keAZhv6GqvtqGnz50u6V9P9jGDwCbZvz4xjYmSTpG5ox7e/plF7C3qj4/Y9MeYGtb3grcOmP8wxk4A3h+xu0bSdIxMJ8PDnsX8CHge0kebGOfBq4EbkqyDXgCuKhtu53BY5BTDB6FvHSYE5YkzW3OuFfVN4EcYfNZs+xfwGVLnJckaQl8h6okdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdWjOuCe5LsnBJA/PGHtLkjuT/LB9f3MbT5IvJplK8lCSdy7n5CVJs5vPlfs/AWcfNrYDuKuqNgN3tXWAc4DN7Ws7cO1wpilJWog5415V3wB+fNjwFmB3W94NnD9j/PoauBdYm2T9kOYqSZqnxd5zX1dVT7blp4B1bXkDsG/Gfvvb2C9Jsj3JZJLJ6enpRU5DkjSbJb+gWlUF1CJ+bmdVTVTVxNjY2FKnIUmaYbFxf/rQ7Zb2/WAbPwBsmrHfxjYmSTqGFhv3PcDWtrwVuHXG+IfbUzNnAM/PuH0jSTpG1sy1Q5IvAe8BTkmyH7gCuBK4Kck24Angorb77cC5wBTwInDpMsxZkjSHOeNeVZccYdNZs+xbwGVLnZQkaWl8h6okdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdWjNqCewmo3vuG0kx338yvNGclxJq4dX7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR3yUUhJK4aPFw+PV+6S1CHjLkkdWpa4Jzk7yQ+STCXZsRzHkCQd2dDjnuQ44O+Bc4BTgUuSnDrs40iSjmw5XlA9HZiqqscAknwZ2AI8sgzHGtkLMJL6McqOLNeLuamq4f7C5ALg7Kr6k7b+IeB3qupjh+23HdjeVn8d+MEiD3kK8KNF/uxK47msPL2cB3guK9VSzuXXqmpstg0jexSyqnYCO5f6e5JMVtXEEKY0cp7LytPLeYDnslIt17ksxwuqB4BNM9Y3tjFJ0jGyHHH/NrA5yVuTnABcDOxZhuNIko5g6LdlquqlJB8D/h04Driuqr4/7OPMsORbOyuI57Ly9HIe4LmsVMtyLkN/QVWSNHq+Q1WSOmTcJalDXcQ9yV8neSjJg0nuSPKro57TYiX52ySPtvO5JcnaUc9pMZJcmOT7SV5OsiofWevlYzSSXJfkYJKHRz2XpUiyKck9SR5pf7cuH/WcFivJSUm+leS77Vw+O/Rj9HDPPckbq+onbflPgVOr6qMjntaiJHkfcHd7YfoqgKr61IintWBJfhN4GfgH4M+ranLEU1qQ9jEa/wW8F9jP4CmwS6pqWd5pvZyS/B7wAnB9Vf3WqOezWEnWA+ur6oEkbwDuB85fpX8mAV5XVS8kOR74JnB5Vd07rGN0ceV+KOzN64BV+2+sqrqjql5qq/cyeJ/AqlNVe6tqse86Xgl+8TEaVfW/wKGP0Vh1quobwI9HPY+lqqonq+qBtvxTYC+wYbSzWpwaeKGtHt++htqtLuIOkORzSfYBHwT+ctTzGZI/Bv511JN4ldoA7Juxvp9VGpIeJRkH3gHcN+KpLFqS45I8CBwE7qyqoZ7Lqol7kv9I8vAsX1sAquozVbUJuAH42NF/22jNdS5tn88ALzE4nxVpPuchDVuS1wM3Ax8/7L/aV5Wq+nlVncbgv85PTzLUW2ar5n+zV1V/MM9dbwBuB65YxuksyVznkuQjwPuBs2oFvyiygD+T1ciP0ViB2v3pm4Ebquqro57PMFTVc0nuAc4Ghvai96q5cj+aJJtnrG4BHh3VXJYqydnAJ4EPVNWLo57Pq5gfo7HCtBchdwF7q+rzo57PUiQZO/QkXJLXMnjhfqjd6uVpmZsZfGzwy8ATwEeralVeZSWZAk4EnmlD967GJ3+S/BHwd8AY8BzwYFX94UgntUBJzgW+wP9/jMbnRjujxUnyJeA9DD5a9mngiqraNdJJLUKSdwP/CXyPwT/rAJ+uqttHN6vFSfLbwG4Gf7deA9xUVX811GP0EHdJ0it1cVtGkvRKxl2SOmTcJalDxl2SOmTcJalDxl2SOmTcJalD/wdJabZ9FEvPGwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = (model(attr_v)).detach().numpy()\n",
    "ans_v_numpy = ans_v.detach().numpy()\n",
    "ret = (out-ans_v_numpy)*10\n",
    "ret =np.round(ret)\n",
    "plt.hist(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rough answer: 928/979 = 94.790603%\n"
     ]
    }
   ],
   "source": [
    "def rough_answer(out:np.ndarray, ans:np.ndarray):\n",
    "    ret = np.round((out-ans)*10.0)\n",
    "    total = out.shape[0]\n",
    "    correct = ((np.abs(ret)<=1)).sum()\n",
    "    return correct, total\n",
    "rc, total = rough_answer(out, ans_v_numpy)\n",
    "print(f\"rough answer: {rc}/{total} = {rc/total*100.0:4f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "073421e87fd17c22a27d2f3fee98ef1fca5a2c350357a034bdf2d94dc9e5a0d1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ai_class': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
